---
title: 'D2SC: Weekly Assignments'
author: "Vidushi Kataria"
date: "`r Sys.Date()`"
output:
  html_document:
    toc: yes
    df_print: paged
  html_notebook:
    toc: yes
    toc_float: yes
---

# Initial Heading
```{r}
library(tidyverse)
library(ggdist)
```

# Assignment 1
Due to my previous programming experience and coursework using R, I knew to use library() to load the tidyverse package.

I did refer to this website, as it gave me a concise description between each function within tidyverse: https://www.tidyverse.org/packages/ 

I checked that it was working by using the command and return buttons on my keyboard to run the code. The console is able to tell me if the code successfully ran or had errors. 

```{r}
help("tidyverse")
```
tidyverse-package {tidyverse}	R Documentation
tidyverse: Easily Install and Load the 'Tidyverse'
Description
logo
The 'tidyverse' is a set of packages that work in harmony because they share common data representations and 'API' design. This package is designed to make it easy to install and load multiple 'tidyverse' packages in a single step. Learn more about the 'tidyverse' at https://www.tidyverse.org.

Author(s)
Maintainer: Hadley Wickham hadley@rstudio.com

Other contributors:

RStudio [copyright holder, funder]

See Also
Useful links:

https://tidyverse.tidyverse.org

https://github.com/tidyverse/tidyverse

Report bugs at https://github.com/tidyverse/tidyverse/issues

# Assignment 2
```{r}
library(dplyr)
getwd()
match_trials <- read_csv("tidy_data/MFIndD_analogy.csv")
```

The qualtrics_id column contains the unique identifier for each participant. 

```{r}
glimpse(match_trials)
```
There are 792 rows and 6 columns. I got this information from the output of using the function glimpse()

```{r}
people <- match_trials %>%
  distinct(qualtrics_id) %>%
  nrow()
people
```

2b: There are 99 unique people in the dataset, as found by counting distinct qualtrics_id values.

```{r}
trial_counts <- match_trials %>%
  count(qualtrics_id) %>%
  pull(n) %>%
  n_distinct()

trial_counts == 1
```

2c: Every unique qualtrics_id has data from the same number of trials, as I counted the number of trials per person, extracted the column, and checked if all values are the same.


# Assignment 3
```{r}
summary_match_trials <- match_trials %>%
  group_by(qualtrics_id) %>%
  summarize(relational_match_count = sum(response_type == "Rel"))

head(summary_match_trials)
```


```{r}
summary_match_trials %>%
  ggplot(aes(x = relational_match_count)) +
  geom_histogram(bindwidth = 1,
                 color = "black", fill = "lightblue") +
  labs(x = "# of Relational Matches Selected",
       y = "# of Participants",
       title = "Relational Matches Selected by Participants")
```
A lot of participants selected 8 relational matches, showing a strong preference for relational matches in many cases. However, there is also a small, yet noticeable peak around 2 relational matches. From what I observe, either participants strongly favored relational matches or preferred object matches, with fewer participants showing moderate selections.


```{r}
reshaped_match_trials <- match_trials %>%
  select(qualtrics_id,
         trial_number,
         response_type) %>%
pivot_wider(names_from = trial_number,
              values_from = response_type,
              names_prefix = "trial") %>%
  drop_na

head(reshaped_match_trials)
```


```{r, eval=FALSE} 
# Error code chunk
reshaped_match_trials <- match_trials %>%
  filter(qualtrics_id,
         trial_number,
         response_type) %>%
  pivot_wider(names_from = trial_number,
              values_from = response_type)
```
R told me I used the wrong function as it does not run character vectors. I searched up all the functions in tidyr and dplyr in the help panel to the right to see what other functions I could use for character vectors.


# Assignment 4
```{r}
rei_data <- read_csv("tidy_data/MFIndD_REI.csv")

head(rei_data)
```
The "response" column is a CHR vector (text) and the "scored_response" column is an INT (numeric) vector. 
They are different even though they both appear to contain numbers is because there is a difference in how the data is represented. The "response" column most likely contains qualitative responses like "Strongly Agree" or "Agree" etc. So this text is not directly numerical.
The "scored_response" column contains numeric values because they might have already been converted and stored as a number.

```{r}
rei_data <- rei_data %>%
  mutate(numeric_response = case_when(
    response == "Strongly Disagree" ~ 1,
    response == "Strongly Agree" ~ 5,
    !is.na(as.numeric(response)) ~ as.numeric(response),
    TRUE ~ NA_real_
  ))

head(rei_data)
```
```{r}
rei_data %>%
  distinct(response)
```


```{r}
rei_data <- rei_data %>%
  mutate(new_scored_response = if_else(!is.na(rev_scoring),
                                      6 - numeric_response,
                                      numeric_response))
head(rei_data)
```

```{r}
comparison <- rei_data %>%
  filter(new_scored_response != scored_response) 

print(comparison)
```


# Assignment 5
```{r}
rei_data <- rei_data %>%
  mutate(new_scored_response = ifelse(rev_scoring == "neg", 6 - new_scored_response, new_scored_response))

rei_summary <- rei_data %>%
  group_by(qualtrics_id, sub_type) %>%
  summarize(score = sum(new_scored_response, na.rm = TRUE), .groups = 'drop')

print(rei_summary)
```


```{r}
rei_na_scores <- rei_summary %>%
  filter(is.na(score))

print(rei_na_scores)
```
This code filters out any rows with missing values in the score column, which helps with identifying which participants have missing data.


```{r}
combined_summary <- rei_summary %>%
  left_join(summary_match_trials, by = "qualtrics_id")

print(combined_summary)
```

```{r}
ggplot(combined_summary, aes(x = score, y = relational_match_count, color = sub_type)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  labs(x = "REI Score", 
       y = "Analogy Score",
       title = "REI vs. Analogy Scores by Sub-Type") +
  theme_minimal() +
  theme(axis.title.x = element_text(color = "blue", size = 12),
        axis.title.y = element_text(color = "blue", size = 12))
```
This plot shows a pretty weak relationship between REI scores and Analogy scores across the four sub-types (EA, EE, RA, RE).
There is a slight positive trend for EA, but the other sub-types show slightly negative trends. There is a lot of noise in the plot which suggests that REI scores do not strongly predict the Analogy score performance.


# Assignment 6
```{r}
probtask <- read_csv("tidy_data/MFIndD_probtask.csv")
```


```{r}
condition <- unique(probtask$condition)
print(condition)
```


```{r}
mean_rt <- numeric(length(condition))
```


```{r}
for (i in seq_along(condition)) {
  mean_rt[i] <- mean(probtask$rt[probtask$condition], na.rm = TRUE)
}
```


```{r}
# table w/ across()

?across()

mean_rt_table <- probtask %>%
  group_by(condition) %>%
  summarise(
    across(c(rt, correct), ~ mean(.x, na.rm = TRUE),
           .names = "mean_{.col}")
  )
mean_rt_table
```


```{r}
# table w/o across()
mean_rt_table_2 <- probtask %>%
  group_by(condition) %>%
  summarise(
    mean_rt = mean(rt, na.rm = TRUE),
    accuracy = mean(correct, na.rm = TRUE)
  )
mean_rt_table_2
```


# Assignment 7

```{r}
probtask %>%
  group_by(condition) %>%
  summarise(across(c(rt, correct), mean)) %>%
  pivot_longer(c(rt, correct)) %>%
  ggplot(aes(y = value, x = condition)) +
  geom_point(color = "red") +
  facet_wrap(~name, scales = "free")
```
1a: From this plot, we can see that the stacked blob format was the most accurate (~0.69), while separated formats performed around ~0.62. Speed wise, separated blobs were fastest (~892 ms), while separated dots were slowest (~847 ms). Based on these observations, we can conclude that the stacked format produced the highest accuracy, while the separated blobs (less accurate) allowed for faster decisions.  

1b: I first noticed the contrasting patterns between accuracy and speed across different visualization types. It seems like the shape of the dots from the left plot are the inverse of the right plot. It could reveal that there might be some trade-off between accuracy and speed. 

1c: Off the bat, I noticed that the visualization type names overlap each other. This would require using a function to rotate or tilt the x-axis labels like theme() and axis.text.x
I also think there should be unit measures for reaction time and accuracy to help the data be portrayed clearer.
Additionally, it is unclear the specific sample population we are targeting. Are there specific proportion differences that were harder to judge in certain formats? 


```{r}
accuracy_sum <- probtask %>%
  group_by(SubID, condition) %>%
  summarise(prop_corr = mean(correct, na.rm = TRUE)) %>%
  group_by(condition) %>%
  summarise(mean_prop_corr = mean(prop_corr, na.rm = TRUE))

accuracy_sum
```


```{r}
accuracy_plot <- accuracy_sum %>%
  ggplot(aes(x = condition, y = mean_prop_corr)) +
  geom_point(color = "blue") +
  labs(title = "Mean Proportion of Correct Responses by Condition",
       x = "Condition", y = "Mean Proportion Correct")

accuracy_plot
```
This plot provides a clearer view of accuracy differences across conditions. It is also a single-focused visualization so it can help the viewer focus on one key measure at a time (in this case just the accuracy data). This plot looks visually less confusing and avoids different scales and metrics to convey the message better.

```{r}
dist_plot <- accuracy_sum %>%
  ggplot(aes(x = condition, y = mean_prop_corr)) +
  stat_dotsinterval(aes(y = mean_prop_corr), 
                    color = "darkblue", # consistent color for interval points
                    slab_fill = "lightblue", # show soft contrast in slab,
                    dotsize = 0.6) + # adjusting dot size 
  labs(title = "Proportion Correct Responses w/ Distributional Info.",
       x = "Condition", y = "Mean Proportion Correct") 

dist_plot
```
The distributional plot provides additional information about the variability of responses for each condition. The circle size can also help the viewer visualize the sample size of each condition. In this case, all the sample sizes across the four conditions look visually equal. 

# Assignment 8

Q1: Added floating toc at the top of notebook

```{r}
summary_probtask <- probtask %>%
  group_by(SubID, condition) %>%
  summarise(
    rt = mean(rt, na.rm = TRUE),
    correct = mean(correct, na.rm = TRUE)) %>%
  ungroup()

head(summary_probtask)
```

```{r}
ggplot(summary_probtask, aes(x = rt, y = correct, color = condition)) +
  geom_point() +
  labs(
    title = "Reaction Time vs Accuracy by Condition",
    x = "Reaction Time (ms)", 
    y = "Proportion Correct") +
  theme_minimal()
```


```{r}
ggplot(summary_probtask, aes(x = rt, y = correct, color = condition)) +
  geom_point() +
  facet_wrap(~condition) +
  labs(
    title = "Reaction Time vs Accuracy by Condition",
    x = "Reaction Time (ms)", 
    y = "Proportion Correct") +
  theme_minimal()
```

**_Observation:_** According to these plots, reaction time tends to have an overall **positive** correlation. As reaction time *increases*, the proportion correct appears to *rise.* (However, there is some variability with *blob_stacked* and *dots_EqSizeSep*).
