---
title: "Weekly Assignment 11"
author:
- name: Vidushi Kataria
  affiliation: '1'
  corresponding: yes
  email: vk412@scarletmail.rutgers.edu
  role:
  - Conceptualization
  - "Writing - Original Draft Preparation"
  - "Writing - Review & Editing"
affiliation:
- id: '1'
  institution: "Rutgers University--New Brunswick"
output:
  papaja::apa6_pdf:
    theme: cerulean
    toc: yes
    toc_float:
      collapsed: yes
  '': default

keywords: keywords
wordcount: X
bibliography: "r-references.bib"
floatsintext: no
linenumbers: yes
draft: no
mask: no
figurelist: no
tablelist: no
footnotelist: no
classoption: man
authornote: |
  Add complete departmental affiliations for each author here. Each new line herein must be indented, like this line.
  Enter author note here.
---

```{r setup, include = FALSE}
library("papaja")
r_refs("r-references.bib")
```


```{r analysis-preferences}
# Seed for random number generation
set.seed(42)
knitr::opts_chunk$set(cache.extra = knitr::rand_seed)
```


```{r}
library(tidyverse)
library(ggdist)

probtask <- read_csv("data/MFIndD_probtask.csv")
```

# Introduction & Research Objectives

Comparing proportions is sometimes very hard! But, even infants seem to be able to do it a little bit. The purpose of this science project was to better understand how well people compare proportions when they are presented in different formats. Such insights contribute to the broader understanding of decision-making processes and the role of stimulus presentation in cognitive tasks.

This study aims to address the following research questions:
1. Does average performance vary across format type?
2. Does average performance vary across numerator congruency status?
3. Does numerator congruency vary across format type? (i.e., is there an interaction)


# Method

```{r participants, include=FALSE}
num_participants <- probtask %>%
  distinct(SubID)

count(num_participants)
```
A total of `r nrow(num_participants)` adults participated in the study. 


First, participants were introduced to a story about a magic ball and that the outcome (i.e., blue or orange) depended on the proportions. They were then asked to compare the proportions of different images.

In other words, participants were shown two images of the same kind at the same time and were asked to decide which had a higher proportion of the shape (or dots) colored in blue.

`r knitr::include_graphics("images_WA10/Probtask_Trial.png")`


There were four different conditions that changed what kinds of images the participants saw:

- divided blobs: blue and orange were entirely separate
- integrated blob: one blob, divided to be part blue and part orange
- separated dots: blue and orange dots were on opposite sides of the image
- integrated dots: blue and orange were intermixed 

`r knitr::include_graphics("images_WA10/Probtask_formats.png")`


# Results

1. Does average performance vary across format type, ignoring all other aspects of the stimuli?
```{r, include=FALSE}
accuracy_sum <- probtask %>%
  group_by(SubID, condition) %>%
  summarise(prop_corr = mean(correct, na.rm = TRUE)) %>%
  group_by(condition) %>%
  summarise(mean_prop_corr = mean(prop_corr, na.rm = TRUE))

accuracy_sum
```


```{r fig-1, fig.cap = "This plot is measuring proportion correct responses with distributional information."}
dist_plot <- accuracy_sum %>%
  ggplot(aes(x = condition, y = mean_prop_corr)) +
  stat_dotsinterval(aes(y = mean_prop_corr), 
                    color = "darkblue", # consistent color for interval points
                    slab_fill = "lightblue", # show soft contrast in slab,
                    dotsize = 0.6) + # adjusting dot size 
  scale_x_discrete(labels = c(
    "blob_shifted" = "Blob Shifted",
    "blob_stacked" = "Blob Stacked",
    "dots_EqSizeRand" = "Dots Eq. Size Rand",
    "dots_EqSizeSep" = "Dots Eq. Size Sep")) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(title = "Proportion Correct Responses w/ Distributional Info.",
       x = "Condition", y = "Mean Proportion Correct") +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

dist_plot
```
When considering only format type (blobs vs. dots), the (`fig-1`) plot reveals no clear systematic difference in average performance between the two formats, as both blob and dot conditions show similar ranges of accuracy (approx 62-69% correct responses) w/ substantial overlap in their distributions.


2. How are reaction time and accuracy related?

```{r include=FALSE}
summary_probtask <- probtask %>%
  group_by(SubID, condition) %>%
  summarise(
    rt = mean(rt, na.rm = TRUE),
    correct = mean(correct, na.rm = TRUE)) %>%
  ungroup()

head(summary_probtask)
```


```{r fig-2, fig.cap = "This plot is measuring reaction time and its impact on accuracy by condition."}
ggplot(summary_probtask, aes(x = rt, y = correct, color = condition)) +
  geom_point() +
  facet_wrap(~condition) +
  scale_x_discrete(labels = c(
    "blob_shifted" = "Blob Shifted",
    "blob_stacked" = "Blob Stacked",
    "dots_EqSizeRand" = "Dots Eq. Size Rand",
    "dots_EqSizeSep" = "Dots Eq. Size Sep")) +
  scale_y_continuous(limits = c(0, 1), expand = c(0, 0)) +
  labs(
    title = "Reaction Time vs Accuracy by Condition",
    x = "Reaction Time (ms)", 
    y = "Proportion Correct") +
  theme_minimal()
```
According to (`fig-2`), reaction time tends to have an overall **positive** correlation. As reaction time *increases*, the proportion correct appears to *rise.* (However, there is some variability with *blob_stacked* and *dots_EqSizeSep*).


3. How does numerator congruency interact with format type?

```{r, include=FALSE, warning=FALSE}
prob_data_mod <- probtask %>%
  separate(left_stim, into = c("left_numerator", "left_other"), sep = "_",
           convert = TRUE) %>%
  separate(right_stim, into = c("right_numerator", "right_other"), sep = "_",
           convert = TRUE) %>%
  mutate(
    left_proportion_value = left_numerator / (left_numerator + left_other),
    right_proportion_value = right_numerator / (right_numerator + right_other)) %>%
  mutate(
    larger_numerator = case_when(left_numerator > right_numerator ~ "left",
                                 TRUE ~ "right")) %>%
  mutate(
    larger_proportion = case_when(
      left_proportion_value > right_proportion_value ~ "left",
      TRUE ~ "right")) %>%
  mutate(
    num_congruent = case_when(
      larger_numerator == larger_proportion ~ TRUE,
      TRUE ~ FALSE))
```


```{r, include=FALSE, warning=FALSE}
plot_data <- prob_data_mod %>%
  mutate(response = as.numeric(response)) %>%
  group_by(SubID, condition, num_congruent) %>%
  summarize(proportion_correct = mean(right_proportion_value + left_proportion_value, na.rm = TRUE), .groups = "drop")
```


```{r fig-3, fig.cap = "This plot is measuring the interaction between format type and numerator congruency."}
plot <- ggplot(plot_data, aes(x = condition, y = proportion_correct,
                      color = num_congruent)) +
  stat_halfeye(alpha = 0.6, side = "both", adjust = 1) +
  scale_x_discrete(labels = c(
    "blob_shifted" = "Blob Shifted",
    "blob_stacked" = "Blob Stacked",
    "dots_EqSizeRand" = "Dots Eq. Size Rand",
    "dots_EqSizeSep" = "Dots Eq. Size Sep")) +
  labs(
    x = "Condition",
    y = "Proportion Correct",
    color = "Numerator Congruency") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))

print(plot)
```
In (`fig-3`), it shows that across all four visualization conditions, participants consistently performed better when the numerator was congruent (TRUE) with the overall ratio compared to when it was incongruent (FALSE). 



## Data analysis
We used `r cite_r("r-references.bib")` for all our analyses.


# Discussion
The three key findings from these three objectives are **format effect** (blobs vs. dots), **speed-accuracy relationship**, and the **numerator congruency effect**. 
The choice between blob or dots format does not appear to meaningfully impact performance accuracy.
There appears to be a speed-accuracy tradeoff, where participants who took more time tended to achieve a higher accuracy. However, there was some variation in the blob_stacked and dots_EqSizeSep formats. 
There was also a consistent pattern that emerged across all visualization types where performance was superior when the numerator aligned with the overall ratio. 


1. Personally, the most annoying thing about this assignment (and making posters/presentations in R in general) is not knowing what is going to print before knitting. I think labeling code chunks can be a pain as well. But at least R tells you immediately after knitting what the issue is.

2. The most satisfying thing about this assignment is seeing everything you intend to print while knitting your poster. 


\newpage

# References

::: {#refs custom-style="Bibliography"}
:::
